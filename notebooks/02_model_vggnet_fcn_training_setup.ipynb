{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train VGGNet and FCN as Segmentation model\n",
    "## Breast-Ultrasound-Segmentation\n",
    "\n",
    "## About Dataset\n",
    "Breast cancer is one of the most common causes of death among women worldwide. Early detection helps in reducing the number of early deaths. The data reviews the medical images of breast cancer using ultrasound scan. Breast Ultrasound Dataset is categorized into three classes: normal, benign, and malignant images. Breast ultrasound images can produce great results in classification, detection, and segmentation of breast cancer when combined with machine learning.\n",
    "\n",
    "### Data\n",
    "The data collected at baseline include breast ultrasound images among women in ages between 25 and 75 years old. This data was collected in 2018. The number of patients is 600 female patients. The dataset consists of 780 images with an average image size of 500*500 pixels. The images are in PNG format. The ground truth images are presented with original images. The images are categorized into three classes, which are normal, benign, and malignant.\n",
    "\n",
    "If you use this dataset, please cite:\n",
    "Al-Dhabyani W, Gomaa M, Khaled H, Fahmy A. Dataset of breast ultrasound images. Data in Brief. 2020 Feb;28:104863. DOI: 10.1016/j.dib.2019.104863."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pyrootutils\n",
    "\n",
    "root = pyrootutils.setup_root(\n",
    "    search_from=os.path.dirname(os.getcwd()),\n",
    "    indicator=[\".git\", \"pyproject.toml\"],\n",
    "    pythonpath=True,\n",
    "    dotenv=True,\n",
    ")\n",
    "\n",
    "if os.getenv(\"DATA_ROOT\") is None:\n",
    "    os.environ[\"DATA_ROOT\"] = f\"{root}/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Found!!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Setup device-agnostic code\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"  # NVIDIA GPU\n",
    "    print(\"GPU Found!!\")\n",
    "else:\n",
    "    raise Exception(\"No GPU Found!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "from hydra import compose, initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # auto reload dotenv\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "# auto reload libs\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'optimizer': {'_target_': 'torch.optim.Adam', '_partial_': True, 'lr': 0.001, 'weight_decay': 0.0, 'betas': [0.9, 0.999]}, 'scheduler': {'func': None}, 'model': {'_target_': 'src.models.vggnet_fcn_segmentation_model.VGGNetFCN16SegmentationModel', 'num_classes': 3}}\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "# Register a resolver for torch dtypes\n",
    "OmegaConf.register_new_resolver(\"torch_dtype\", lambda name: getattr(torch, name))\n",
    "\n",
    "with initialize(config_path=\"../configs\", job_name=\"training_setup\", version_base=None):\n",
    "    cfg: DictConfig = compose(config_name=\"train.yaml\")\n",
    "    # print(OmegaConf.to_yaml(cfg))\n",
    "    print(cfg.models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['normal', 'malignant', 'benign'], 3, tensor([1.9774, 1.2494, 0.5903]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_module = hydra.utils.instantiate(cfg.datamodule)\n",
    "\n",
    "class_weights = data_module.class_weights\n",
    "class_names = data_module.classes\n",
    "num_classes = len(class_names)\n",
    "class_names, num_classes, class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# # 1. Select a very small subset of your data\n",
    "train_dataset, val_dataset = data_module.split_and_preprocess_datasets()\n",
    "\n",
    "subset_indices = list(range(10))  # Select first 10 samples\n",
    "train_small_dataset = Subset(train_dataset, subset_indices)\n",
    "val_small_dataset = Subset(val_dataset, subset_indices)\n",
    "train_dl = DataLoader(train_small_dataset, batch_size=5, shuffle=False)\n",
    "val_dl = DataLoader(val_small_dataset, batch_size=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 224, 224]) torch.Size([5, 1, 224, 224]) torch.Size([5])\n",
      "images:torch.float32, -2.1179039478302, 2.640000104904175\n",
      "masks torch.uint8, 0, 1\n",
      "labels torch.int64, 1, 2\n"
     ]
    }
   ],
   "source": [
    "for images, targets in train_dl:\n",
    "    print(images.shape, targets[\"masks\"].shape, targets[\"labels\"].shape)\n",
    "\n",
    "    print(f\"images:{images.dtype}, {images[0].min()}, {images[0].max()}\")\n",
    "    print(\n",
    "        f'masks {targets[\"masks\"].dtype}, {targets[\"masks\"][0].min()}, {targets[\"masks\"][0].max()}'\n",
    "    )\n",
    "    print(\n",
    "        f'labels {targets[\"labels\"].dtype}, {targets[\"labels\"].min()}, {targets[\"labels\"].max()}'\n",
    "    )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 224, 224]) torch.Size([5, 1, 224, 224]) torch.Size([5])\n",
      "images:torch.float32, -2.1179039478302, 2.5354254245758057\n",
      "masks torch.uint8, 0, 1\n",
      "labels torch.int64, 0, 2\n"
     ]
    }
   ],
   "source": [
    "for _images, _targets in val_dl:\n",
    "    print(_images.shape, _targets[\"masks\"].shape, _targets[\"labels\"].shape)\n",
    "\n",
    "    print(f\"images:{_images[0].dtype}, {_images[0].min()}, {_images[0].max()}\")\n",
    "    print(f'masks {_targets[\"masks\"].dtype}, {_targets[\"masks\"].min()}, {_targets[\"masks\"].max()}')\n",
    "    print(\n",
    "        f'labels {_targets[\"labels\"].dtype}, {_targets[\"labels\"].min()}, {_targets[\"labels\"].max()}'\n",
    "    )\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and training the FCN8 model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.gpu_utils import DeviceDataLoader, get_default_device, to_device\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "device = get_default_device()\n",
    "\n",
    "gpu_weights = to_device(class_weights, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): VGGNetFCN8SegmentationModel(\n",
       "    (segmentation_criterion): CrossEntropyLoss()\n",
       "    (classification_criterion): CrossEntropyLoss()\n",
       "    (encoder): VGGNetEncoder(\n",
       "      (vgg): VGG(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): ReLU(inplace=True)\n",
       "          (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (6): ReLU(inplace=True)\n",
       "          (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (8): ReLU(inplace=True)\n",
       "          (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (11): ReLU(inplace=True)\n",
       "          (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (13): ReLU(inplace=True)\n",
       "          (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (15): ReLU(inplace=True)\n",
       "          (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (18): ReLU(inplace=True)\n",
       "          (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (20): ReLU(inplace=True)\n",
       "          (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (22): ReLU(inplace=True)\n",
       "          (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (25): ReLU(inplace=True)\n",
       "          (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (27): ReLU(inplace=True)\n",
       "          (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (29): ReLU(inplace=True)\n",
       "          (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "      )\n",
       "      (final_conv): Sequential(\n",
       "        (0): Conv2d(512, 4096, kernel_size=(7, 7), stride=(1, 1), padding=same)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(4096, 4096, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (decoder): FCN8Decoder(\n",
       "      (encoder): VGGNetEncoder(\n",
       "        (vgg): VGG(\n",
       "          (features): Sequential(\n",
       "            (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (3): ReLU(inplace=True)\n",
       "            (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "            (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (6): ReLU(inplace=True)\n",
       "            (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (8): ReLU(inplace=True)\n",
       "            (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "            (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (11): ReLU(inplace=True)\n",
       "            (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (13): ReLU(inplace=True)\n",
       "            (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (15): ReLU(inplace=True)\n",
       "            (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "            (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (18): ReLU(inplace=True)\n",
       "            (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (20): ReLU(inplace=True)\n",
       "            (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (22): ReLU(inplace=True)\n",
       "            (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "            (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (25): ReLU(inplace=True)\n",
       "            (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (27): ReLU(inplace=True)\n",
       "            (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (29): ReLU(inplace=True)\n",
       "            (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          )\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "        )\n",
       "        (final_conv): Sequential(\n",
       "          (0): Conv2d(512, 4096, kernel_size=(7, 7), stride=(1, 1), padding=same)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(4096, 4096, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "          (3): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (upsample_5): Sequential(\n",
       "        (0): ConvTranspose2d(4096, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (bn5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (upsample_4): Sequential(\n",
       "        (0): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (upsample3): Sequential(\n",
       "        (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (upsample2): Sequential(\n",
       "        (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (upsample1): Sequential(\n",
       "        (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (mask_classifier): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (image_classifier): Sequential(\n",
       "        (0): AdaptiveAvgPool2d(output_size=1)\n",
       "        (1): Flatten(start_dim=1, end_dim=-1)\n",
       "        (2): Linear(in_features=4096, out_features=512, bias=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Linear(in_features=256, out_features=3, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models.vggnet_fcn8_segmentation_model import VGGNetFCN8SegmentationModel\n",
    "\n",
    "model = VGGNetFCN8SegmentationModel(\n",
    "    num_classes=num_classes,\n",
    "    vggnet_type=\"vgg16\",\n",
    "    # class_weights=gpu_weights,\n",
    "    segmentation_criterion=nn.CrossEntropyLoss(),\n",
    "    classification_criterion=nn.CrossEntropyLoss(weight=gpu_weights),\n",
    ")\n",
    "model = torch.compile(model)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0280, -0.0601, -0.0400], grad_fn=<SelectBackward0>) torch.Size([5, 3])\n",
      "tensor([[[ 0.2217,  0.4533, -0.2362,  ..., -0.0991,  0.1820, -0.3981],\n",
      "         [-0.4762,  0.0610, -0.2882,  ..., -0.3821, -0.1835,  0.1531],\n",
      "         [ 0.2427, -0.0021, -0.8194,  ..., -0.0435,  0.0468, -0.5177],\n",
      "         ...,\n",
      "         [-1.1953,  0.7841, -1.6076,  ...,  0.2585, -0.5988, -0.6560],\n",
      "         [ 0.1092,  0.3181,  0.4308,  ...,  0.1388,  0.6095,  0.9809],\n",
      "         [ 0.0249, -1.6544,  0.0896,  ..., -1.7758, -1.1326,  0.4621]],\n",
      "\n",
      "        [[-0.0490,  0.0566,  0.0811,  ...,  0.3426, -0.0279,  0.1367],\n",
      "         [ 0.0827,  0.4742,  1.1841,  ...,  0.1159, -0.2437,  0.2171],\n",
      "         [-0.1569,  0.6254,  0.3810,  ..., -0.2084, -0.1497, -0.0424],\n",
      "         ...,\n",
      "         [ 0.3016,  0.5003,  0.7117,  ...,  0.6720, -0.8637, -0.6873],\n",
      "         [ 0.2908, -0.2330,  0.6262,  ..., -1.0957, -0.2563,  0.2996],\n",
      "         [-0.3628,  0.0792,  0.7188,  ...,  0.8495,  0.2817,  0.5719]],\n",
      "\n",
      "        [[ 0.4832,  0.2969,  0.0160,  ...,  0.2114,  0.7245,  0.4730],\n",
      "         [-0.0323, -1.2963,  0.3075,  ...,  0.7328, -0.4848,  1.5341],\n",
      "         [ 0.2319, -0.2826, -1.3568,  ...,  0.3064,  0.4050,  0.0837],\n",
      "         ...,\n",
      "         [ 0.1733, -1.0766,  1.8936,  ...,  0.4291,  0.6273,  0.6680],\n",
      "         [ 0.1633,  0.6311,  0.3082,  ..., -1.2019,  0.5077, -0.5738],\n",
      "         [-0.4428, -0.2982,  0.2136,  ..., -0.2654, -0.6386, -0.1425]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "torch.Size([5, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_dl:\n",
    "    # print(images.shape, labels)\n",
    "    out = model(images)\n",
    "    # l = labels['labels'][0]\n",
    "    # print(l , torch.argmax(l))\n",
    "    print(out[\"labels\"][0], out[\"labels\"].shape)\n",
    "    print(out[\"masks\"][0])\n",
    "    print(out[\"masks\"].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, val_dl) -> dict[str, float]:\n",
    "    model.eval()  # set model to evaluate mode\n",
    "    outputs = [model.validation_step(batch) for batch in val_dl]\n",
    "    return model.validation_epoch_end(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    validation_dataloader,\n",
    "    epochs: int = 2,\n",
    "    lr: float = 1e-3,\n",
    "    opt_func=torch.optim.Adam,\n",
    ") -> list[dict[str, float]]:\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        train_accuracies = []\n",
    "\n",
    "        train_losses = []\n",
    "        for batch in train_dataloader:\n",
    "            step_output = model.training_step(batch)\n",
    "            loss = step_output[\"loss\"]\n",
    "            train_acc = step_output[\"train_acc\"]\n",
    "            # Detach loss and accuracy before appending to avoid holding onto computation graph\n",
    "            train_losses.append(loss.detach())\n",
    "            train_accuracies.append(train_acc.detach())  # Assuming train_acc is a tensor\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # Validation Phase\n",
    "        result = evaluate(model, validation_dataloader)\n",
    "        result[\"train_loss\"] = torch.stack(train_losses).mean().item()\n",
    "        result[\"train_acc\"] = torch.stack(train_accuracies).mean().item()\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate(model, val_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Training Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving data and model into memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train only Classification head "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): VGGNetFCN8SegmentationModel(\n",
       "    (segmentation_criterion): CrossEntropyLoss()\n",
       "    (classification_criterion): CrossEntropyLoss()\n",
       "    (encoder): VGGNetEncoder(\n",
       "      (vgg): VGG(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): ReLU(inplace=True)\n",
       "          (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (6): ReLU(inplace=True)\n",
       "          (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (8): ReLU(inplace=True)\n",
       "          (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (11): ReLU(inplace=True)\n",
       "          (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (13): ReLU(inplace=True)\n",
       "          (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (15): ReLU(inplace=True)\n",
       "          (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (18): ReLU(inplace=True)\n",
       "          (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (20): ReLU(inplace=True)\n",
       "          (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (22): ReLU(inplace=True)\n",
       "          (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (25): ReLU(inplace=True)\n",
       "          (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (27): ReLU(inplace=True)\n",
       "          (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (29): ReLU(inplace=True)\n",
       "          (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "      )\n",
       "      (final_conv): Sequential(\n",
       "        (0): Conv2d(512, 4096, kernel_size=(7, 7), stride=(1, 1), padding=same)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(4096, 4096, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (decoder): FCN8Decoder(\n",
       "      (encoder): VGGNetEncoder(\n",
       "        (vgg): VGG(\n",
       "          (features): Sequential(\n",
       "            (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (3): ReLU(inplace=True)\n",
       "            (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "            (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (6): ReLU(inplace=True)\n",
       "            (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (8): ReLU(inplace=True)\n",
       "            (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "            (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (11): ReLU(inplace=True)\n",
       "            (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (13): ReLU(inplace=True)\n",
       "            (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (15): ReLU(inplace=True)\n",
       "            (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "            (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (18): ReLU(inplace=True)\n",
       "            (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (20): ReLU(inplace=True)\n",
       "            (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (22): ReLU(inplace=True)\n",
       "            (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "            (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (25): ReLU(inplace=True)\n",
       "            (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (27): ReLU(inplace=True)\n",
       "            (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (29): ReLU(inplace=True)\n",
       "            (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          )\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "        )\n",
       "        (final_conv): Sequential(\n",
       "          (0): Conv2d(512, 4096, kernel_size=(7, 7), stride=(1, 1), padding=same)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(4096, 4096, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "          (3): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (upsample_5): Sequential(\n",
       "        (0): ConvTranspose2d(4096, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (bn5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (upsample_4): Sequential(\n",
       "        (0): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (upsample3): Sequential(\n",
       "        (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (upsample2): Sequential(\n",
       "        (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (upsample1): Sequential(\n",
       "        (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (mask_classifier): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (image_classifier): Sequential(\n",
       "        (0): AdaptiveAvgPool2d(output_size=1)\n",
       "        (1): Flatten(start_dim=1, end_dim=-1)\n",
       "        (2): Linear(in_features=4096, out_features=512, bias=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Linear(in_features=256, out_features=3, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dl = DeviceDataLoader(train_dl, device)\n",
    "val_dl = DeviceDataLoader(val_dl, device)\n",
    "to_device(model, device)\n",
    "# train_dl.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20  # Train for significantly more epochs than usual\n",
    "\n",
    "# history = fit(\n",
    "#     model=model,\n",
    "#     train_dataloader=train_dl,\n",
    "#     validation_dataloader=val_dl,\n",
    "#     epochs=num_epochs,\n",
    "#     lr=1e-4,\n",
    "#     opt_func=optim.Adam,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.visualizations import plot_losses\n",
    "\n",
    "# plot_losses(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "\n",
    "\n",
    "class MetricKey(enum.Enum):\n",
    "    \"\"\"Keys for metrics dictionary.\"\"\"\n",
    "\n",
    "    # Classification\n",
    "    CLS_LOSS = \"cls_loss\"\n",
    "    CLS_ACC = \"cls_acc\"\n",
    "    VAL_CLS_LOSS = \"val_cls_loss\"\n",
    "    VAL_CLS_ACC = \"val_cls_acc\"\n",
    "\n",
    "    # Segmentation\n",
    "    SEG_LOSS = \"seg_loss\"\n",
    "    SEG_DICE = \"seg_dice\"\n",
    "    VAL_SEG_LOSS = \"val_seg_loss\"\n",
    "    VAL_SEG_DICE = \"val_seg_dice\"\n",
    "\n",
    "\n",
    "def fit_seg(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    validation_dataloader,\n",
    "    epochs: int = 2,\n",
    "    lr: float = 1e-3,\n",
    "    opt_func=torch.optim.Adam,\n",
    ") -> list[dict[str, float]]:\n",
    "    history = []\n",
    "    result = {}\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        masks_dice_sc = []\n",
    "        train_accuracies = []\n",
    "        loss = []\n",
    "        masks_losses = []\n",
    "        for batch in train_dataloader:\n",
    "            step_output = model.training_step(batch)\n",
    "            masks_loss = step_output[f\"{MetricKey.SEG_LOSS.value}\"]\n",
    "            _masks_dice_sc = step_output[f\"{MetricKey.SEG_DICE.value}\"]\n",
    "\n",
    "            train_acc = step_output[f\"{MetricKey.CLS_ACC.value}\"]\n",
    "            loss = step_output[f\"{MetricKey.CLS_LOSS.value}\"]\n",
    "            # Detach loss and accuracy before appending to avoid holding onto computation graph\n",
    "            train_losses.append(loss.detach())\n",
    "            masks_losses.append(masks_loss.detach())  # Assuming train_acc is a tensor\n",
    "            masks_dice_sc.append(_masks_dice_sc)  # Assuming train_acc is a tensor\n",
    "            train_accuracies.append(train_acc.detach())  # Assuming train_acc is a tensor\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Validation Phase\n",
    "        result = evaluate(model, validation_dataloader)\n",
    "        result[f\"{MetricKey.CLS_LOSS.value}\"] = torch.stack(train_losses).mean().item()\n",
    "        result[f\"{MetricKey.SEG_LOSS.value}\"] = torch.stack(masks_losses).mean().item()\n",
    "        result[f\"{MetricKey.SEG_DICE.value}\"] = torch.stack(masks_dice_sc).mean().item()\n",
    "        result[f\"{MetricKey.CLS_ACC.value}\"] = torch.stack(train_accuracies).mean().item()\n",
    "        model.epoch_end(epoch, result)\n",
    "        # history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0] Validation Results: val_cls_loss=1.0882, val_cls_acc=0.6000, val_seg_loss=1.1820, val_seg_dice=0.1332\n",
      "Epoch [1] Validation Results: val_cls_loss=1.0850, val_cls_acc=0.6000, val_seg_loss=1.1868, val_seg_dice=0.1338\n",
      "Epoch [2] Validation Results: val_cls_loss=1.0815, val_cls_acc=0.6000, val_seg_loss=1.1911, val_seg_dice=0.1340\n",
      "Epoch [3] Validation Results: val_cls_loss=1.0773, val_cls_acc=0.6000, val_seg_loss=1.1948, val_seg_dice=0.1338\n",
      "Epoch [4] Validation Results: val_cls_loss=1.0729, val_cls_acc=0.6000, val_seg_loss=1.1980, val_seg_dice=0.1335\n",
      "Epoch [5] Validation Results: val_cls_loss=1.0682, val_cls_acc=0.6000, val_seg_loss=1.2007, val_seg_dice=0.1331\n",
      "Epoch [6] Validation Results: val_cls_loss=1.0633, val_cls_acc=0.6000, val_seg_loss=1.2029, val_seg_dice=0.1327\n",
      "Epoch [7] Validation Results: val_cls_loss=1.0580, val_cls_acc=0.6000, val_seg_loss=1.2048, val_seg_dice=0.1323\n",
      "Epoch [8] Validation Results: val_cls_loss=1.0520, val_cls_acc=0.6000, val_seg_loss=1.2063, val_seg_dice=0.1320\n",
      "Epoch [9] Validation Results: val_cls_loss=1.0441, val_cls_acc=0.6000, val_seg_loss=1.2078, val_seg_dice=0.1319\n",
      "Epoch [10] Validation Results: val_cls_loss=1.0292, val_cls_acc=0.6000, val_seg_loss=1.2091, val_seg_dice=0.1318\n",
      "Epoch [11] Validation Results: val_cls_loss=1.0044, val_cls_acc=0.6000, val_seg_loss=1.2106, val_seg_dice=0.1316\n",
      "Epoch [12] Validation Results: val_cls_loss=0.9711, val_cls_acc=0.7000, val_seg_loss=1.2125, val_seg_dice=0.1314\n",
      "Epoch [13] Validation Results: val_cls_loss=0.9321, val_cls_acc=0.7000, val_seg_loss=1.2144, val_seg_dice=0.1314\n",
      "Epoch [14] Validation Results: val_cls_loss=0.8879, val_cls_acc=0.7000, val_seg_loss=1.2163, val_seg_dice=0.1312\n",
      "Epoch [15] Validation Results: val_cls_loss=0.8390, val_cls_acc=0.8000, val_seg_loss=1.2183, val_seg_dice=0.1314\n",
      "Epoch [16] Validation Results: val_cls_loss=0.7853, val_cls_acc=0.8000, val_seg_loss=1.2207, val_seg_dice=0.1313\n",
      "Epoch [17] Validation Results: val_cls_loss=0.7246, val_cls_acc=0.7000, val_seg_loss=1.2229, val_seg_dice=0.1315\n",
      "Epoch [18] Validation Results: val_cls_loss=0.6670, val_cls_acc=0.7000, val_seg_loss=1.2252, val_seg_dice=0.1316\n",
      "Epoch [19] Validation Results: val_cls_loss=0.6250, val_cls_acc=0.6000, val_seg_loss=1.2273, val_seg_dice=0.1317\n",
      "Epoch [20] Validation Results: val_cls_loss=0.5845, val_cls_acc=0.6000, val_seg_loss=1.2293, val_seg_dice=0.1318\n",
      "Epoch [21] Validation Results: val_cls_loss=0.4988, val_cls_acc=0.6000, val_seg_loss=1.2306, val_seg_dice=0.1319\n",
      "Epoch [22] Validation Results: val_cls_loss=0.4122, val_cls_acc=0.9000, val_seg_loss=1.2320, val_seg_dice=0.1319\n",
      "Epoch [23] Validation Results: val_cls_loss=0.3594, val_cls_acc=0.9000, val_seg_loss=1.2331, val_seg_dice=0.1320\n",
      "Epoch [24] Validation Results: val_cls_loss=0.3153, val_cls_acc=0.9000, val_seg_loss=1.2343, val_seg_dice=0.1321\n",
      "Epoch [25] Validation Results: val_cls_loss=0.3084, val_cls_acc=0.8000, val_seg_loss=1.2353, val_seg_dice=0.1320\n",
      "Epoch [26] Validation Results: val_cls_loss=0.3577, val_cls_acc=0.8000, val_seg_loss=1.2361, val_seg_dice=0.1320\n",
      "Epoch [27] Validation Results: val_cls_loss=0.3990, val_cls_acc=0.8000, val_seg_loss=1.2366, val_seg_dice=0.1318\n",
      "Epoch [28] Validation Results: val_cls_loss=0.4558, val_cls_acc=0.8000, val_seg_loss=1.2368, val_seg_dice=0.1318\n",
      "Epoch [29] Validation Results: val_cls_loss=0.4445, val_cls_acc=0.8000, val_seg_loss=1.2365, val_seg_dice=0.1318\n",
      "Epoch [30] Validation Results: val_cls_loss=0.5401, val_cls_acc=0.7000, val_seg_loss=1.2365, val_seg_dice=0.1318\n",
      "Epoch [31] Validation Results: val_cls_loss=0.5460, val_cls_acc=0.7000, val_seg_loss=1.2364, val_seg_dice=0.1319\n",
      "Epoch [32] Validation Results: val_cls_loss=0.4802, val_cls_acc=0.8000, val_seg_loss=1.2357, val_seg_dice=0.1319\n",
      "Epoch [33] Validation Results: val_cls_loss=0.8166, val_cls_acc=0.6000, val_seg_loss=1.2355, val_seg_dice=0.1318\n",
      "Epoch [34] Validation Results: val_cls_loss=0.6603, val_cls_acc=0.7000, val_seg_loss=1.2353, val_seg_dice=0.1318\n",
      "Epoch [35] Validation Results: val_cls_loss=0.5509, val_cls_acc=0.8000, val_seg_loss=1.2347, val_seg_dice=0.1319\n",
      "Epoch [36] Validation Results: val_cls_loss=0.5962, val_cls_acc=0.7000, val_seg_loss=1.2337, val_seg_dice=0.1317\n",
      "Epoch [37] Validation Results: val_cls_loss=0.6396, val_cls_acc=0.6000, val_seg_loss=1.2333, val_seg_dice=0.1317\n",
      "Epoch [38] Validation Results: val_cls_loss=0.3583, val_cls_acc=0.9000, val_seg_loss=1.2329, val_seg_dice=0.1318\n",
      "Epoch [39] Validation Results: val_cls_loss=0.3114, val_cls_acc=0.9000, val_seg_loss=1.2328, val_seg_dice=0.1319\n",
      "Epoch [40] Validation Results: val_cls_loss=0.3931, val_cls_acc=0.8000, val_seg_loss=1.2328, val_seg_dice=0.1319\n",
      "Epoch [41] Validation Results: val_cls_loss=0.4692, val_cls_acc=0.8000, val_seg_loss=1.2327, val_seg_dice=0.1318\n",
      "Epoch [42] Validation Results: val_cls_loss=0.4557, val_cls_acc=0.8000, val_seg_loss=1.2329, val_seg_dice=0.1317\n",
      "Epoch [43] Validation Results: val_cls_loss=0.4349, val_cls_acc=0.8000, val_seg_loss=1.2327, val_seg_dice=0.1316\n",
      "Epoch [44] Validation Results: val_cls_loss=0.4157, val_cls_acc=0.8000, val_seg_loss=1.2328, val_seg_dice=0.1316\n",
      "Epoch [45] Validation Results: val_cls_loss=0.3999, val_cls_acc=0.8000, val_seg_loss=1.2331, val_seg_dice=0.1318\n",
      "Epoch [46] Validation Results: val_cls_loss=0.3857, val_cls_acc=0.8000, val_seg_loss=1.2331, val_seg_dice=0.1318\n",
      "Epoch [47] Validation Results: val_cls_loss=0.3728, val_cls_acc=0.8000, val_seg_loss=1.2331, val_seg_dice=0.1318\n",
      "Epoch [48] Validation Results: val_cls_loss=0.3594, val_cls_acc=0.8000, val_seg_loss=1.2333, val_seg_dice=0.1317\n",
      "Epoch [49] Validation Results: val_cls_loss=0.3470, val_cls_acc=0.9000, val_seg_loss=1.2335, val_seg_dice=0.1317\n",
      "Epoch [50] Validation Results: val_cls_loss=0.3381, val_cls_acc=0.9000, val_seg_loss=1.2337, val_seg_dice=0.1316\n",
      "Epoch [51] Validation Results: val_cls_loss=0.3311, val_cls_acc=0.9000, val_seg_loss=1.2339, val_seg_dice=0.1316\n",
      "Epoch [52] Validation Results: val_cls_loss=0.3246, val_cls_acc=0.9000, val_seg_loss=1.2341, val_seg_dice=0.1317\n",
      "Epoch [53] Validation Results: val_cls_loss=0.3191, val_cls_acc=0.9000, val_seg_loss=1.2342, val_seg_dice=0.1318\n",
      "Epoch [54] Validation Results: val_cls_loss=0.3160, val_cls_acc=0.9000, val_seg_loss=1.2341, val_seg_dice=0.1319\n",
      "Epoch [55] Validation Results: val_cls_loss=0.3146, val_cls_acc=0.9000, val_seg_loss=1.2340, val_seg_dice=0.1319\n",
      "Epoch [56] Validation Results: val_cls_loss=0.3134, val_cls_acc=0.9000, val_seg_loss=1.2334, val_seg_dice=0.1317\n",
      "Epoch [57] Validation Results: val_cls_loss=0.3131, val_cls_acc=0.9000, val_seg_loss=1.2333, val_seg_dice=0.1319\n",
      "Epoch [58] Validation Results: val_cls_loss=0.3132, val_cls_acc=0.9000, val_seg_loss=1.2333, val_seg_dice=0.1318\n",
      "Epoch [59] Validation Results: val_cls_loss=0.3132, val_cls_acc=0.9000, val_seg_loss=1.2332, val_seg_dice=0.1318\n",
      "Epoch [60] Validation Results: val_cls_loss=0.3134, val_cls_acc=0.9000, val_seg_loss=1.2332, val_seg_dice=0.1319\n",
      "Epoch [61] Validation Results: val_cls_loss=0.3139, val_cls_acc=0.9000, val_seg_loss=1.2335, val_seg_dice=0.1317\n",
      "Epoch [62] Validation Results: val_cls_loss=0.3143, val_cls_acc=0.9000, val_seg_loss=1.2338, val_seg_dice=0.1317\n",
      "Epoch [63] Validation Results: val_cls_loss=0.3137, val_cls_acc=0.9000, val_seg_loss=1.2337, val_seg_dice=0.1317\n",
      "Epoch [64] Validation Results: val_cls_loss=0.3127, val_cls_acc=0.9000, val_seg_loss=1.2339, val_seg_dice=0.1317\n",
      "Epoch [65] Validation Results: val_cls_loss=0.3126, val_cls_acc=0.9000, val_seg_loss=1.2339, val_seg_dice=0.1318\n",
      "Epoch [66] Validation Results: val_cls_loss=0.3131, val_cls_acc=0.9000, val_seg_loss=1.2338, val_seg_dice=0.1317\n",
      "Epoch [67] Validation Results: val_cls_loss=0.3139, val_cls_acc=0.9000, val_seg_loss=1.2336, val_seg_dice=0.1317\n",
      "Epoch [68] Validation Results: val_cls_loss=0.3153, val_cls_acc=0.9000, val_seg_loss=1.2336, val_seg_dice=0.1318\n",
      "Epoch [69] Validation Results: val_cls_loss=0.3169, val_cls_acc=0.9000, val_seg_loss=1.2335, val_seg_dice=0.1317\n"
     ]
    }
   ],
   "source": [
    "# TODO: try using dice loss\n",
    "history = fit_seg(\n",
    "    model=model,\n",
    "    train_dataloader=train_dl,\n",
    "    validation_dataloader=val_dl,\n",
    "    epochs=num_epochs + 50,\n",
    "    lr=1e-5,\n",
    "    opt_func=optim.Adam,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Loss vs. NO. of epochs')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_losses = [x[\"train_loss\"] for x in history]\n",
    "\n",
    "plt.plot(train_losses, \"-bx\")\n",
    "\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.grid()\n",
    "plt.legend([\"train_loss\"])\n",
    "plt.title(\"Loss vs. NO. of epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 224, 224]), torch.Size([32, 3, 224, 224]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Example predictions (logits) and targets\n",
    "predictions = torch.randn(32, 3, 224, 224)\n",
    "targets = torch.randint(0, 3, (32, 224, 224), dtype=torch.long)\n",
    "\n",
    "targets.shape, predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3901805877685547\n",
      "2232118.5\n"
     ]
    }
   ],
   "source": [
    "# Calculate the loss using the functional form\n",
    "loss = F.cross_entropy(predictions, targets)\n",
    "print(loss.item())\n",
    "\n",
    "# You can also specify arguments directly in the function call\n",
    "loss_with_reduction = F.cross_entropy(predictions, targets, reduction=\"sum\")\n",
    "print(loss_with_reduction.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
