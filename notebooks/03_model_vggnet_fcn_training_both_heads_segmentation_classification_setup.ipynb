{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup to train full model with findinng proper weights to train each head (Classification + Segmentation)\n",
    "## Breast-Ultrasound-Segmentation\n",
    "\n",
    "## About Dataset\n",
    "Breast cancer is one of the most common causes of death among women worldwide. Early detection helps in reducing the number of early deaths. The data reviews the medical images of breast cancer using ultrasound scan. Breast Ultrasound Dataset is categorized into three classes: normal, benign, and malignant images. Breast ultrasound images can produce great results in classification, detection, and segmentation of breast cancer when combined with machine learning.\n",
    "\n",
    "### Data\n",
    "The data collected at baseline include breast ultrasound images among women in ages between 25 and 75 years old. This data was collected in 2018. The number of patients is 600 female patients. The dataset consists of 780 images with an average image size of 500*500 pixels. The images are in PNG format. The ground truth images are presented with original images. The images are categorized into three classes, which are normal, benign, and malignant.\n",
    "\n",
    "If you use this dataset, please cite:\n",
    "Al-Dhabyani W, Gomaa M, Khaled H, Fahmy A. Dataset of breast ultrasound images. Data in Brief. 2020 Feb;28:104863. DOI: 10.1016/j.dib.2019.104863."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pyrootutils\n",
    "\n",
    "root = pyrootutils.setup_root(\n",
    "    search_from=os.path.dirname(os.getcwd()),\n",
    "    indicator=[\".git\", \"pyproject.toml\"],\n",
    "    pythonpath=True,\n",
    "    dotenv=True,\n",
    ")\n",
    "\n",
    "if os.getenv(\"DATA_ROOT\") is None:\n",
    "    os.environ[\"DATA_ROOT\"] = f\"{root}/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Found!!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Setup device-agnostic code\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"  # NVIDIA GPU\n",
    "    print(\"GPU Found!!\")\n",
    "else:\n",
    "    raise Exception(\"No GPU Found!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import hydra\n",
    "from hydra import compose, initialize\n",
    "\n",
    "log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # auto reload dotenv\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "# auto reload libs\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "# Register a resolver for torch dtypes\n",
    "OmegaConf.register_new_resolver(\"torch_dtype\", lambda name: getattr(torch, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'task_name': 'overfitting-tests', 'tags': ['dev'], 'train': True, 'test': False, 'ckpt_path': None, 'seed': 42, 'data': {'dataset': {'_target_': 'src.datamodules.components.breast_cancer_dataset.BreastCancerDataset', 'data_dir': '${data.dataset_dir}', 'dataset_url': '${data.url}'}, 'train_shared_transforms': [{'_target_': 'torchvision.transforms.v2.Resize', 'size': [224, 224], 'antialias': True}, {'_target_': 'torchvision.transforms.v2.RandomHorizontalFlip'}, {'_target_': 'torchvision.transforms.v2.RandomVerticalFlip'}, {'_target_': 'torchvision.transforms.v2.ElasticTransform', 'alpha': 50.0, 'sigma': 5.0}], 'train_image_trasforms': [{'_target_': 'torchvision.transforms.v2.ToDtype', 'dtype': \"${torch_dtype:'float32'}\", 'scale': True}, {'_target_': 'torchvision.transforms.v2.Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}], 'train_masks_transforms': [{'_target_': 'torchvision.transforms.v2.ToDtype', 'dtype': \"${torch_dtype:'float32'}\", 'scale': False}], 'val_shared_transforms': [{'_target_': 'torchvision.transforms.v2.Resize', 'size': [224, 224]}], 'val_image_transforms': [{'_target_': 'torchvision.transforms.v2.ToDtype', 'dtype': \"${torch_dtype:'float32'}\", 'scale': True}, {'_target_': 'torchvision.transforms.v2.Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}], 'val_masks_transforms': [{'_target_': 'torchvision.transforms.v2.ToDtype', 'dtype': \"${torch_dtype:'float32'}\", 'scale': False}], 'dataset_name': 'breast-ultrasound-images-dataset', 'dataset_dir': 'data/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT', 'url': 'https://www.kaggle.com/datasets/aryashah2k/breast-ultrasound-images-dataset'}, 'trainer': {'default_root_dir': '${paths.output_dir}', 'max_epochs': 10, 'accelerator': 'gpu', 'devices': 0}, 'paths': {'root_dir': '${oc.env:PROJECT_ROOT}', 'root_data_dir': '${oc.env:DATA_ROOT}', 'log_dir': '${paths.root_dir}/logs/', 'output_dir': '${hydra:runtime.output_dir}', 'work_dir': '${hydra:runtime.cwd}'}, 'extras': {'ignore_warnings': False, 'enforce_tags': True, 'print_config': True}, 'datamodule': {'batch_size': 32, 'num_workers': 16, 'pin_memory': True, 'persistent_workers': True, 'val_split': 0.2, 'random_state': '${seed}', 'data': {'dataset': {'_target_': 'src.datamodules.components.breast_cancer_dataset.BreastCancerDataset', 'data_dir': '${data.dataset_dir}', 'dataset_url': '${data.url}'}, 'train_shared_transforms': [{'_target_': 'torchvision.transforms.v2.Resize', 'size': [224, 224], 'antialias': True}, {'_target_': 'torchvision.transforms.v2.RandomHorizontalFlip'}, {'_target_': 'torchvision.transforms.v2.RandomVerticalFlip'}, {'_target_': 'torchvision.transforms.v2.ElasticTransform', 'alpha': 50.0, 'sigma': 5.0}], 'train_image_trasforms': [{'_target_': 'torchvision.transforms.v2.ToDtype', 'dtype': \"${torch_dtype:'float32'}\", 'scale': True}, {'_target_': 'torchvision.transforms.v2.Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}], 'train_masks_transforms': [{'_target_': 'torchvision.transforms.v2.ToDtype', 'dtype': \"${torch_dtype:'float32'}\", 'scale': False}], 'val_shared_transforms': [{'_target_': 'torchvision.transforms.v2.Resize', 'size': [224, 224]}], 'val_image_transforms': [{'_target_': 'torchvision.transforms.v2.ToDtype', 'dtype': \"${torch_dtype:'float32'}\", 'scale': True}, {'_target_': 'torchvision.transforms.v2.Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}], 'val_masks_transforms': [{'_target_': 'torchvision.transforms.v2.ToDtype', 'dtype': \"${torch_dtype:'float32'}\", 'scale': False}], 'dataset_name': 'breast-ultrasound-images-dataset', 'dataset_dir': 'data/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT', 'url': 'https://www.kaggle.com/datasets/aryashah2k/breast-ultrasound-images-dataset'}, '_target_': 'src.datamodules.breast_cancer_dataloader_module.BreastCancerDataLoaderModule'}, 'models': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 0.003}, 'scheduler': {'func': None}, 'model': {'_target_': 'src.models.vggnet_fcn_segmentation_model.VGGNetFCNSegmentationModel', 'segmentation_criterion': None, 'classification_criterion': None, 'seg_num_classes': 1, 'cls_num_classes': 3, 'seg_weight': 0.95, 'cls_weight': 0.05, 'vggnet_type': 'vgg16', 'fcn_type': 'fcn8'}}, 'losses': {'segmentation_criterion': {'_target_': 'src.losses.dice_loss.SoftDiceLoss'}, 'classification_criterion': {'_target_': 'torch.nn.CrossEntropyLoss', 'weight': None}}}\n"
     ]
    }
   ],
   "source": [
    "with initialize(config_path=\"../configs\", job_name=\"training_setup\", version_base=None):\n",
    "    cfg: DictConfig = compose(config_name=\"train.yaml\")\n",
    "    # print(OmegaConf.to_yaml(cfg))\n",
    "    print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['normal', 'malignant', 'benign'], 3, tensor([1.9774, 1.2494, 0.5903]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_module = hydra.utils.instantiate(cfg.datamodule)\n",
    "\n",
    "class_weights = data_module.class_weights\n",
    "class_names = data_module.classes\n",
    "num_classes = len(class_names)\n",
    "class_names, num_classes, class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled dataset size: 315\n"
     ]
    }
   ],
   "source": [
    "train_dl, val_dl = data_module.get_sampled_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224]) torch.Size([32, 1, 224, 224]) torch.Size([32])\n",
      "images:torch.float32, -2.1179039478302, 2.239128828048706\n",
      "masks torch.uint8, 0, 0\n",
      "labels torch.int64, 0, 2\n"
     ]
    }
   ],
   "source": [
    "images, targets = next(iter(train_dl))\n",
    "print(images.shape, targets[\"masks\"].shape, targets[\"labels\"].shape)\n",
    "\n",
    "print(f\"images:{images.dtype}, {images[0].min()}, {images[0].max()}\")\n",
    "print(f'masks {targets[\"masks\"].dtype}, {targets[\"masks\"][0].min()}, {targets[\"masks\"][0].max()}')\n",
    "print(f'labels {targets[\"labels\"].dtype}, {targets[\"labels\"].min()}, {targets[\"labels\"].max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224]) torch.Size([32, 1, 224, 224]) torch.Size([32])\n",
      "images:torch.float32, -2.1179039478302, 2.640000104904175\n",
      "masks torch.uint8, 0, 1\n",
      "labels torch.int64, 0, 2\n"
     ]
    }
   ],
   "source": [
    "_images, _targets = next(iter(val_dl))\n",
    "\n",
    "print(_images.shape, _targets[\"masks\"].shape, _targets[\"labels\"].shape)\n",
    "\n",
    "print(f\"images:{_images[0].dtype}, {_images[0].min()}, {_images[0].max()}\")\n",
    "print(f'masks {_targets[\"masks\"].dtype}, {_targets[\"masks\"].min()}, {_targets[\"masks\"].max()}')\n",
    "print(f'labels {_targets[\"labels\"].dtype}, {_targets[\"labels\"].min()}, {_targets[\"labels\"].max()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and training the FCN8 model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.9774, 1.2494, 0.5903])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segmentation_criterion = hydra.utils.instantiate(cfg.losses.segmentation_criterion)\n",
    "classification_criterion = hydra.utils.instantiate(\n",
    "    cfg.losses.classification_criterion, weight=class_weights\n",
    ")\n",
    "classification_criterion.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "from src.utils.gpu_utils import DeviceDataLoader, get_default_device, to_device\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "device = get_default_device()\n",
    "\n",
    "gpu_weights = to_device(class_weights, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 0.003}, 'scheduler': {'func': None}, 'model': {'_target_': 'src.models.vggnet_fcn_segmentation_model.VGGNetFCNSegmentationModel', 'segmentation_criterion': None, 'classification_criterion': None, 'seg_num_classes': 1, 'cls_num_classes': 3, 'seg_weight': 0.95, 'cls_weight': 0.05, 'vggnet_type': 'vgg16', 'fcn_type': 'fcn8'}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = hydra.utils.instantiate(\n",
    "    cfg.models.model,\n",
    "    segmentation_criterion=segmentation_criterion,\n",
    "    classification_criterion=classification_criterion,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): VGGNetFCNSegmentationModel(\n",
       "    (segmentation_criterion): SoftDiceLoss()\n",
       "    (classification_criterion): CrossEntropyLoss()\n",
       "    (cls_auroc): MulticlassAUROC()\n",
       "    (encoder): VGGNetEncoder(\n",
       "      (vgg): VGG(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): ReLU(inplace=True)\n",
       "          (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (6): ReLU(inplace=True)\n",
       "          (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (8): ReLU(inplace=True)\n",
       "          (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (11): ReLU(inplace=True)\n",
       "          (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (13): ReLU(inplace=True)\n",
       "          (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (15): ReLU(inplace=True)\n",
       "          (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (18): ReLU(inplace=True)\n",
       "          (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (20): ReLU(inplace=True)\n",
       "          (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (22): ReLU(inplace=True)\n",
       "          (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (25): ReLU(inplace=True)\n",
       "          (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (27): ReLU(inplace=True)\n",
       "          (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (29): ReLU(inplace=True)\n",
       "          (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "      )\n",
       "      (final_conv): Sequential(\n",
       "        (0): Conv2d(512, 4096, kernel_size=(7, 7), stride=(1, 1), padding=same)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(4096, 4096, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (decoder): FCN8Decoder(\n",
       "      (upsample_5): Sequential(\n",
       "        (0): ConvTranspose2d(4096, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (bn5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (upsample_4): Sequential(\n",
       "        (0): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (upsample3): Sequential(\n",
       "        (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (upsample2): Sequential(\n",
       "        (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (upsample1): Sequential(\n",
       "        (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (fcn8_mask_segmentation): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (classification_head): ClassificationHead(\n",
       "      (head): Sequential(\n",
       "        (0): AdaptiveAvgPool2d(output_size=1)\n",
       "        (1): Flatten(start_dim=1, end_dim=-1)\n",
       "        (2): Linear(in_features=4096, out_features=512, bias=True)\n",
       "        (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Dropout(p=0.5, inplace=False)\n",
       "        (6): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (7): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (8): ReLU(inplace=True)\n",
       "        (9): Dropout(p=0.5, inplace=False)\n",
       "        (10): Linear(in_features=256, out_features=3, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.compile(model)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0485, -0.0661, -0.0109]) torch.Size([32, 3])\n",
      "tensor([[[-0.1150, -0.1726, -0.1475,  ..., -0.1662, -0.1486, -0.0939],\n",
      "         [-0.0909, -0.1174, -0.1678,  ..., -0.1083, -0.1540, -0.0844],\n",
      "         [-0.1602, -0.1890, -0.1252,  ..., -0.2339, -0.1050, -0.1089],\n",
      "         ...,\n",
      "         [-0.0912, -0.2368, -0.2086,  ..., -0.1636, -0.0718, -0.1317],\n",
      "         [-0.0959, -0.2723, -0.2127,  ..., -0.1324, -0.1128, -0.1306],\n",
      "         [-0.1376, -0.1892, -0.2293,  ..., -0.1414, -0.1422, -0.1360]]])\n",
      "torch.Size([32, 1, 224, 224])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "inputs: \n",
       "  ['image_input': Tensor('float32', (-1, 3, 224, 224))]\n",
       "outputs: \n",
       "  ['output': {labels: Array(Array(float)) (required), masks: Array(Array(Array(Array(float)))) (required)} (required)]\n",
       "params: \n",
       "  None"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlflow.models import infer_signature\n",
    "\n",
    "task_name = cfg.task_name\n",
    "mlflow.set_experiment(f\"overfitting-{task_name}\")\n",
    "run = mlflow.start_run()\n",
    "model.eval()  # This ensures layers like Dropout and BatchNorm behave correctly for inference and saves computation.\n",
    "with torch.no_grad():\n",
    "    images, labels = next(iter(train_dl))\n",
    "    # print(images.shape, labels)\n",
    "    out = model(images)\n",
    "    # l = labels['labels'][0]\n",
    "    # print(l , torch.argmax(l))\n",
    "    print(out[\"labels\"][0], out[\"labels\"].shape)\n",
    "    print(out[\"masks\"][0])\n",
    "    print(out[\"masks\"].shape)\n",
    "    signature = infer_signature(\n",
    "        model_input={\"image_input\": images.numpy()},\n",
    "        model_output={\"output\": {\"masks\": out[\"masks\"].numpy(), \"labels\": out[\"labels\"].numpy()}},\n",
    "    )\n",
    "signature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Training Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving data and model into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DeviceDataLoader(train_dl, device)\n",
    "val_dl = DeviceDataLoader(val_dl, device)\n",
    "to_device(model, device)\n",
    "# train_dl.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfiting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = hydra.utils.instantiate(cfg.models.optimizer, params=model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'root_dir': '${oc.env:PROJECT_ROOT}', 'root_data_dir': '${oc.env:DATA_ROOT}', 'log_dir': '${paths.root_dir}/logs/', 'output_dir': '${hydra:runtime.output_dir}', 'work_dir': '${hydra:runtime.cwd}'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.paths.output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "from src.utils.train_utils import fit\n",
    "\n",
    "EPOCHS = cfg.trainer.max_epochs\n",
    "mlflow.log_params({\"epochs\": EPOCHS})\n",
    "mlflow.log_params({\"batch_size\": cfg.datamodule.batch_size})\n",
    "mlflow.log_params({\"optimizer\": cfg.models.optimizer.values()})\n",
    "# Log model summary.\n",
    "with open(\"model_summary.txt\", \"w\") as f:\n",
    "    f.write(str(summary(model)))\n",
    "mlflow.log_artifact(\"model_summary.txt\")\n",
    "history = fit(\n",
    "    model=model,\n",
    "    train_dataloader=train_dl,\n",
    "    validation_dataloader=val_dl,\n",
    "    epochs=EPOCHS,\n",
    "    optimizer=optimizer,\n",
    "    device_type=device.type,\n",
    "    dtype=torch.float16,\n",
    "    reduce_lr_on_plateau=torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, factor=0.1, patience=5\n",
    "    ),\n",
    ")\n",
    "# saving the trained model\n",
    "mlflow.pytorch.log_model(model, \"model\", signature=signature)\n",
    "mlflow.log_metrics(history[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seg_losses = [x[\"seg_loss\"] for x in history]\n",
    "seg_dice = [x[\"seg_dice\"] for x in history]\n",
    "\n",
    "plt.plot(seg_losses, \"-bx\")\n",
    "plt.plot(seg_dice, \"-rx\")\n",
    "\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.grid()\n",
    "plt.legend([\"seg_loss\", \"seg_dice\"])\n",
    "plt.title(\"Loss vs. NO. of epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
