{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train VGGNet and FCN as Segmentation model\n",
    "## Breast-Ultrasound-Segmentation\n",
    "\n",
    "## About Dataset\n",
    "Breast cancer is one of the most common causes of death among women worldwide. Early detection helps in reducing the number of early deaths. The data reviews the medical images of breast cancer using ultrasound scan. Breast Ultrasound Dataset is categorized into three classes: normal, benign, and malignant images. Breast ultrasound images can produce great results in classification, detection, and segmentation of breast cancer when combined with machine learning.\n",
    "\n",
    "### Data\n",
    "The data collected at baseline include breast ultrasound images among women in ages between 25 and 75 years old. This data was collected in 2018. The number of patients is 600 female patients. The dataset consists of 780 images with an average image size of 500*500 pixels. The images are in PNG format. The ground truth images are presented with original images. The images are categorized into three classes, which are normal, benign, and malignant.\n",
    "\n",
    "If you use this dataset, please cite:\n",
    "Al-Dhabyani W, Gomaa M, Khaled H, Fahmy A. Dataset of breast ultrasound images. Data in Brief. 2020 Feb;28:104863. DOI: 10.1016/j.dib.2019.104863."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pyrootutils\n",
    "\n",
    "root = pyrootutils.setup_root(\n",
    "    search_from=os.path.dirname(os.getcwd()),\n",
    "    indicator=[\".git\", \"pyproject.toml\"],\n",
    "    pythonpath=True,\n",
    "    dotenv=True,\n",
    ")\n",
    "\n",
    "if os.getenv(\"DATA_ROOT\") is None:\n",
    "    os.environ[\"DATA_ROOT\"] = f\"{root}/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Found!!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Setup device-agnostic code\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"  # NVIDIA GPU\n",
    "    print(\"GPU Found!!\")\n",
    "else:\n",
    "    raise Exception(\"No GPU Found!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "from hydra import compose, initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # auto reload dotenv\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "# auto reload libs\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'optimizer': {'_target_': 'torch.optim.Adam', '_partial_': True, 'lr': 0.001, 'weight_decay': 0.0, 'betas': [0.9, 0.999]}, 'scheduler': {'func': None}, 'model': {'_target_': 'src.models.vggnet_fcn_segmentation_model.VGGNetFCN16SegmentationModel', 'num_classes': 3}}\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "# Register a resolver for torch dtypes\n",
    "OmegaConf.register_new_resolver(\"torch_dtype\", lambda name: getattr(torch, name))\n",
    "\n",
    "with initialize(config_path=\"../configs\", job_name=\"training_setup\", version_base=None):\n",
    "    cfg: DictConfig = compose(config_name=\"train.yaml\")\n",
    "    # print(OmegaConf.to_yaml(cfg))\n",
    "    print(cfg.models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['normal', 'malignant', 'benign'], 3, tensor([1.9774, 1.2494, 0.5903]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_module = hydra.utils.instantiate(cfg.datamodule)\n",
    "\n",
    "class_weights = data_module.class_weights\n",
    "class_names = data_module.classes\n",
    "num_classes = len(class_names)\n",
    "class_names, num_classes, class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# # 1. Select a very small subset of your data\n",
    "train_dataset, val_dataset = data_module.split_and_preprocess_datasets()\n",
    "\n",
    "subset_indices = list(range(10))  # Select first 10 samples\n",
    "train_small_dataset = Subset(train_dataset, subset_indices)\n",
    "val_small_dataset = Subset(val_dataset, subset_indices)\n",
    "train_dl = DataLoader(train_small_dataset, batch_size=5, shuffle=False)\n",
    "val_dl = DataLoader(val_small_dataset, batch_size=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 224, 224]) torch.Size([5, 1, 224, 224]) torch.Size([5])\n",
      "images:torch.float32, -2.1179039478302, 2.6225709915161133\n",
      "masks torch.uint8, 0, 1\n",
      "labels torch.int64, 1, 2\n"
     ]
    }
   ],
   "source": [
    "for images, targets in train_dl:\n",
    "    print(images.shape, targets[\"masks\"].shape, targets[\"labels\"].shape)\n",
    "\n",
    "    print(f\"images:{images.dtype}, {images[0].min()}, {images[0].max()}\")\n",
    "    print(\n",
    "        f'masks {targets[\"masks\"].dtype}, {targets[\"masks\"][0].min()}, {targets[\"masks\"][0].max()}'\n",
    "    )\n",
    "    print(\n",
    "        f'labels {targets[\"labels\"].dtype}, {targets[\"labels\"].min()}, {targets[\"labels\"].max()}'\n",
    "    )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 224, 224]) torch.Size([5, 1, 224, 224]) torch.Size([5])\n",
      "images:torch.float32, -2.1179039478302, 2.5354254245758057\n",
      "masks torch.uint8, 0, 1\n",
      "labels torch.int64, 0, 2\n"
     ]
    }
   ],
   "source": [
    "for _images, _targets in val_dl:\n",
    "    print(_images.shape, _targets[\"masks\"].shape, _targets[\"labels\"].shape)\n",
    "\n",
    "    print(f\"images:{_images[0].dtype}, {_images[0].min()}, {_images[0].max()}\")\n",
    "    print(f'masks {_targets[\"masks\"].dtype}, {_targets[\"masks\"].min()}, {_targets[\"masks\"].max()}')\n",
    "    print(\n",
    "        f'labels {_targets[\"labels\"].dtype}, {_targets[\"labels\"].min()}, {_targets[\"labels\"].max()}'\n",
    "    )\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and training the FCN8 model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.gpu_utils import DeviceDataLoader, get_default_device, to_device\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "device = get_default_device()\n",
    "\n",
    "gpu_weights = to_device(class_weights, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): VGGNetFCN16SegmentationModel(\n",
       "    (encoder): VGGNetEncoder(\n",
       "      (vgg): VGG(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): ReLU(inplace=True)\n",
       "          (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (6): ReLU(inplace=True)\n",
       "          (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (8): ReLU(inplace=True)\n",
       "          (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (11): ReLU(inplace=True)\n",
       "          (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (13): ReLU(inplace=True)\n",
       "          (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (15): ReLU(inplace=True)\n",
       "          (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (18): ReLU(inplace=True)\n",
       "          (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (20): ReLU(inplace=True)\n",
       "          (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (22): ReLU(inplace=True)\n",
       "          (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (25): ReLU(inplace=True)\n",
       "          (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (27): ReLU(inplace=True)\n",
       "          (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (29): ReLU(inplace=True)\n",
       "          (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "      )\n",
       "      (final_conv): Sequential(\n",
       "        (0): Conv2d(512, 4096, kernel_size=(7, 7), stride=(1, 1), padding=same)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(4096, 4096, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (decoder): FCN16Decoder(\n",
       "      (encoder): VGGNetEncoder(\n",
       "        (vgg): VGG(\n",
       "          (features): Sequential(\n",
       "            (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (3): ReLU(inplace=True)\n",
       "            (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "            (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (6): ReLU(inplace=True)\n",
       "            (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (8): ReLU(inplace=True)\n",
       "            (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "            (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (11): ReLU(inplace=True)\n",
       "            (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (13): ReLU(inplace=True)\n",
       "            (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (15): ReLU(inplace=True)\n",
       "            (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "            (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (18): ReLU(inplace=True)\n",
       "            (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (20): ReLU(inplace=True)\n",
       "            (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (22): ReLU(inplace=True)\n",
       "            (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "            (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (25): ReLU(inplace=True)\n",
       "            (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (27): ReLU(inplace=True)\n",
       "            (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (29): ReLU(inplace=True)\n",
       "            (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          )\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "        )\n",
       "        (final_conv): Sequential(\n",
       "          (0): Conv2d(512, 4096, kernel_size=(7, 7), stride=(1, 1), padding=same)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(4096, 4096, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "          (3): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (upsample_5): Sequential(\n",
       "        (0): ConvTranspose2d(4096, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (bn5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (upsample_4): Sequential(\n",
       "        (0): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (upsample3): Sequential(\n",
       "        (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (upsample2): Sequential(\n",
       "        (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (upsample1): Sequential(\n",
       "        (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (mask_classifier): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (image_classifier): Sequential(\n",
       "        (0): AdaptiveAvgPool2d(output_size=1)\n",
       "        (1): Flatten(start_dim=1, end_dim=-1)\n",
       "        (2): Linear(in_features=4096, out_features=512, bias=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Linear(in_features=256, out_features=3, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models.vggnet_fcn16_segmentation_model import VGGNetFCN16SegmentationModel\n",
    "\n",
    "model = VGGNetFCN16SegmentationModel(\n",
    "    num_classes=num_classes, vggnet_type=\"vgg16\", class_weights=gpu_weights\n",
    ")\n",
    "model = torch.compile(model)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0178,  0.0420,  0.0440], grad_fn=<SelectBackward0>) torch.Size([5, 3])\n",
      "tensor([[[-0.6220,  0.7259, -1.3855,  ...,  0.5466, -0.3727, -0.1547],\n",
      "         [ 1.5213,  1.3550,  1.0672,  ..., -0.1229,  0.0301, -0.4407],\n",
      "         [-0.7953,  1.3074,  0.1862,  ...,  0.8725, -1.1757, -0.1379],\n",
      "         ...,\n",
      "         [ 0.5386, -1.7544,  0.3659,  ...,  1.2633,  0.2619, -0.2495],\n",
      "         [-0.2249, -0.2340,  0.3166,  ...,  0.4324, -0.2928, -0.3038],\n",
      "         [-0.0977,  0.1719,  0.7858,  ...,  0.1595, -0.0399, -0.2169]],\n",
      "\n",
      "        [[-0.6621,  0.3084, -1.6984,  ..., -0.3331, -0.2171,  0.1832],\n",
      "         [ 2.4377,  4.0421,  3.9546,  ...,  1.5136,  0.5262,  0.0863],\n",
      "         [ 0.6439,  0.9135,  1.3605,  ...,  1.4076,  1.1149,  0.2864],\n",
      "         ...,\n",
      "         [ 0.1575, -0.9485,  1.1830,  ..., -1.1196, -0.1106, -0.5340],\n",
      "         [ 0.0803, -0.5012, -0.6488,  ...,  0.6220,  0.0528, -0.2015],\n",
      "         [-0.2362,  0.4922,  0.4942,  ..., -0.1537, -0.2174, -0.6524]],\n",
      "\n",
      "        [[ 0.2847, -0.2758, -0.1208,  ...,  0.2330,  0.3597, -0.4525],\n",
      "         [ 0.1139, -2.8789, -3.3235,  ..., -1.5359, -0.2568,  0.3704],\n",
      "         [-0.6922, -0.5137, -0.0836,  ...,  0.4010, -1.0191, -1.2318],\n",
      "         ...,\n",
      "         [-0.2069,  0.9864, -0.3205,  ..., -0.1677,  0.1642,  0.0674],\n",
      "         [ 0.0582,  1.2684, -0.1209,  ..., -0.1598,  0.0403,  0.3344],\n",
      "         [ 0.0937,  0.3468,  0.2197,  ...,  0.4389,  0.0297,  0.2259]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "torch.Size([5, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_dl:\n",
    "    # print(images.shape, labels)\n",
    "    out = model(images)\n",
    "    # l = labels['labels'][0]\n",
    "    # print(l , torch.argmax(l))\n",
    "    print(out[\"labels\"][0], out[\"labels\"].shape)\n",
    "    print(out[\"masks\"][0])\n",
    "    print(out[\"masks\"].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, val_dl) -> dict[str, float]:\n",
    "    model.eval()  # set model to evaluate mode\n",
    "    outputs = [model.validation_step(batch) for batch in val_dl]\n",
    "    return model.validation_epoch_end(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    validation_dataloader,\n",
    "    epochs: int = 2,\n",
    "    lr: float = 1e-3,\n",
    "    opt_func=torch.optim.Adam,\n",
    ") -> list[dict[str, float]]:\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        train_accuracies = []\n",
    "\n",
    "        train_losses = []\n",
    "        for batch in train_dataloader:\n",
    "            step_output = model.training_step(batch)\n",
    "            loss = step_output[\"loss\"]\n",
    "            train_acc = step_output[\"train_acc\"]\n",
    "            # Detach loss and accuracy before appending to avoid holding onto computation graph\n",
    "            train_losses.append(loss.detach())\n",
    "            train_accuracies.append(train_acc.detach())  # Assuming train_acc is a tensor\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # Validation Phase\n",
    "        result = evaluate(model, validation_dataloader)\n",
    "        result[\"train_loss\"] = torch.stack(train_losses).mean().item()\n",
    "        result[\"train_acc\"] = torch.stack(train_accuracies).mean().item()\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate(model, val_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Training Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving data and model into memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train only Classification head "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): VGGNetFCN16SegmentationModel(\n",
       "    (encoder): VGGNetEncoder(\n",
       "      (vgg): VGG(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): ReLU(inplace=True)\n",
       "          (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (6): ReLU(inplace=True)\n",
       "          (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (8): ReLU(inplace=True)\n",
       "          (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (11): ReLU(inplace=True)\n",
       "          (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (13): ReLU(inplace=True)\n",
       "          (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (15): ReLU(inplace=True)\n",
       "          (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (18): ReLU(inplace=True)\n",
       "          (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (20): ReLU(inplace=True)\n",
       "          (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (22): ReLU(inplace=True)\n",
       "          (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (25): ReLU(inplace=True)\n",
       "          (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (27): ReLU(inplace=True)\n",
       "          (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (29): ReLU(inplace=True)\n",
       "          (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "      )\n",
       "      (final_conv): Sequential(\n",
       "        (0): Conv2d(512, 4096, kernel_size=(7, 7), stride=(1, 1), padding=same)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(4096, 4096, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (decoder): FCN16Decoder(\n",
       "      (encoder): VGGNetEncoder(\n",
       "        (vgg): VGG(\n",
       "          (features): Sequential(\n",
       "            (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (3): ReLU(inplace=True)\n",
       "            (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "            (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (6): ReLU(inplace=True)\n",
       "            (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (8): ReLU(inplace=True)\n",
       "            (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "            (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (11): ReLU(inplace=True)\n",
       "            (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (13): ReLU(inplace=True)\n",
       "            (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (15): ReLU(inplace=True)\n",
       "            (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "            (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (18): ReLU(inplace=True)\n",
       "            (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (20): ReLU(inplace=True)\n",
       "            (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (22): ReLU(inplace=True)\n",
       "            (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "            (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (25): ReLU(inplace=True)\n",
       "            (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (27): ReLU(inplace=True)\n",
       "            (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (29): ReLU(inplace=True)\n",
       "            (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          )\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "        )\n",
       "        (final_conv): Sequential(\n",
       "          (0): Conv2d(512, 4096, kernel_size=(7, 7), stride=(1, 1), padding=same)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(4096, 4096, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "          (3): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (upsample_5): Sequential(\n",
       "        (0): ConvTranspose2d(4096, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (bn5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (upsample_4): Sequential(\n",
       "        (0): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (upsample3): Sequential(\n",
       "        (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (upsample2): Sequential(\n",
       "        (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (upsample1): Sequential(\n",
       "        (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (mask_classifier): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (image_classifier): Sequential(\n",
       "        (0): AdaptiveAvgPool2d(output_size=1)\n",
       "        (1): Flatten(start_dim=1, end_dim=-1)\n",
       "        (2): Linear(in_features=4096, out_features=512, bias=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Linear(in_features=256, out_features=3, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dl = DeviceDataLoader(train_dl, device)\n",
    "val_dl = DeviceDataLoader(val_dl, device)\n",
    "to_device(model, device)\n",
    "# train_dl.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20  # Train for significantly more epochs than usual\n",
    "\n",
    "# history = fit(\n",
    "#     model=model,\n",
    "#     train_dataloader=train_dl,\n",
    "#     validation_dataloader=val_dl,\n",
    "#     epochs=num_epochs,\n",
    "#     lr=1e-4,\n",
    "#     opt_func=optim.Adam,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.visualizations import plot_losses\n",
    "\n",
    "# plot_losses(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "\n",
    "\n",
    "class MetricKey(enum.Enum):\n",
    "    # classification\n",
    "    CLS_LOSS = \"cls_loss\"\n",
    "    CLS_TRAIN_ACC = \"cls_train_acc\"\n",
    "    VAL_CLS_LOSS = \"cls_val_loss\"\n",
    "    VAL_CLS_ACC = \"cls_val_acc\"\n",
    "\n",
    "    # segmentation\n",
    "    MASKS_LOSS = \"masks_loss\"\n",
    "    MASKS_DICE_SCORE = \"masks_dice_score\"\n",
    "    VAL_MASK_LOSS = \"val_mask_loss\"\n",
    "    VAL_DICE_SCORE = \"val_dice_score\"\n",
    "\n",
    "\n",
    "def fit_seg(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    validation_dataloader,\n",
    "    epochs: int = 2,\n",
    "    lr: float = 1e-3,\n",
    "    opt_func=torch.optim.Adam,\n",
    ") -> list[dict[str, float]]:\n",
    "    history = []\n",
    "    result = {}\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        masks_dice_sc = []\n",
    "        train_accuracies = []\n",
    "        loss = []\n",
    "        masks_losses = []\n",
    "        for batch in train_dataloader:\n",
    "            step_output = model.training_step(batch)\n",
    "            masks_loss = step_output[f\"{MetricKey.MASKS_LOSS.value}\"]\n",
    "            _masks_dice_sc = step_output[f\"{MetricKey.MASKS_DICE_SCORE.value}\"]\n",
    "\n",
    "            train_acc = step_output[f\"{MetricKey.CLS_TRAIN_ACC.value}\"]\n",
    "            loss = step_output[f\"{MetricKey.CLS_LOSS.value}\"]\n",
    "            # Detach loss and accuracy before appending to avoid holding onto computation graph\n",
    "            train_losses.append(loss.detach())\n",
    "            masks_losses.append(masks_loss.detach())  # Assuming train_acc is a tensor\n",
    "            masks_dice_sc.append(_masks_dice_sc)  # Assuming train_acc is a tensor\n",
    "            train_accuracies.append(train_acc.detach())  # Assuming train_acc is a tensor\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Validation Phase\n",
    "        result = evaluate(model, validation_dataloader)\n",
    "        result[f\"{MetricKey.CLS_LOSS.value}\"] = torch.stack(train_losses).mean().item()\n",
    "        result[f\"{MetricKey.MASKS_LOSS.value}\"] = torch.stack(masks_losses).mean().item()\n",
    "        result[f\"{MetricKey.MASKS_DICE_SCORE.value}\"] = torch.stack(masks_dice_sc).mean().item()\n",
    "        result[f\"{MetricKey.CLS_TRAIN_ACC.value}\"] = torch.stack(train_accuracies).mean().item()\n",
    "        model.epoch_end(epoch, result)\n",
    "        # history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], masks_loss: 1.1372,masks_dice_score: nan,\n",
      "Epoch [1], masks_loss: 1.1374,masks_dice_score: nan,\n",
      "Epoch [2], masks_loss: 1.1368,masks_dice_score: nan,\n",
      "Epoch [3], masks_loss: 1.1382,masks_dice_score: nan,\n",
      "Epoch [4], masks_loss: 1.1373,masks_dice_score: nan,\n",
      "Epoch [5], masks_loss: 1.1363,masks_dice_score: nan,\n",
      "Epoch [6], masks_loss: 1.1371,masks_dice_score: nan,\n",
      "Epoch [7], masks_loss: 1.1372,masks_dice_score: nan,\n",
      "Epoch [8], masks_loss: 1.1390,masks_dice_score: nan,\n",
      "Epoch [9], masks_loss: 1.1384,masks_dice_score: nan,\n",
      "Epoch [10], masks_loss: 1.1380,masks_dice_score: nan,\n",
      "Epoch [11], masks_loss: 1.1390,masks_dice_score: nan,\n",
      "Epoch [12], masks_loss: 1.1392,masks_dice_score: nan,\n",
      "Epoch [13], masks_loss: 1.1393,masks_dice_score: nan,\n",
      "Epoch [14], masks_loss: 1.1386,masks_dice_score: nan,\n",
      "Epoch [15], masks_loss: 1.1397,masks_dice_score: nan,\n",
      "Epoch [16], masks_loss: 1.1391,masks_dice_score: nan,\n",
      "Epoch [17], masks_loss: 1.1411,masks_dice_score: nan,\n",
      "Epoch [18], masks_loss: 1.1411,masks_dice_score: nan,\n",
      "Epoch [19], masks_loss: 1.1416,masks_dice_score: nan,\n",
      "Epoch [20], masks_loss: 1.1417,masks_dice_score: nan,\n",
      "Epoch [21], masks_loss: 1.1422,masks_dice_score: nan,\n",
      "Epoch [22], masks_loss: 1.1428,masks_dice_score: nan,\n",
      "Epoch [23], masks_loss: 1.1428,masks_dice_score: nan,\n",
      "Epoch [24], masks_loss: 1.1439,masks_dice_score: nan,\n",
      "Epoch [25], masks_loss: 1.1440,masks_dice_score: nan,\n",
      "Epoch [26], masks_loss: 1.1442,masks_dice_score: nan,\n",
      "Epoch [27], masks_loss: 1.1432,masks_dice_score: nan,\n",
      "Epoch [28], masks_loss: 1.1444,masks_dice_score: nan,\n",
      "Epoch [29], masks_loss: 1.1442,masks_dice_score: nan,\n",
      "Epoch [30], masks_loss: 1.1455,masks_dice_score: nan,\n",
      "Epoch [31], masks_loss: 1.1448,masks_dice_score: nan,\n",
      "Epoch [32], masks_loss: 1.1447,masks_dice_score: nan,\n",
      "Epoch [33], masks_loss: 1.1450,masks_dice_score: nan,\n",
      "Epoch [34], masks_loss: 1.1456,masks_dice_score: nan,\n",
      "Epoch [35], masks_loss: 1.1457,masks_dice_score: nan,\n",
      "Epoch [36], masks_loss: 1.1456,masks_dice_score: nan,\n",
      "Epoch [37], masks_loss: 1.1453,masks_dice_score: nan,\n",
      "Epoch [38], masks_loss: 1.1452,masks_dice_score: nan,\n",
      "Epoch [39], masks_loss: 1.1449,masks_dice_score: nan,\n",
      "Epoch [40], masks_loss: 1.1454,masks_dice_score: nan,\n",
      "Epoch [41], masks_loss: 1.1455,masks_dice_score: nan,\n",
      "Epoch [42], masks_loss: 1.1448,masks_dice_score: nan,\n",
      "Epoch [43], masks_loss: 1.1453,masks_dice_score: nan,\n",
      "Epoch [44], masks_loss: 1.1456,masks_dice_score: nan,\n",
      "Epoch [45], masks_loss: 1.1461,masks_dice_score: nan,\n",
      "Epoch [46], masks_loss: 1.1449,masks_dice_score: nan,\n",
      "Epoch [47], masks_loss: 1.1463,masks_dice_score: nan,\n",
      "Epoch [48], masks_loss: 1.1456,masks_dice_score: nan,\n",
      "Epoch [49], masks_loss: 1.1458,masks_dice_score: nan,\n",
      "Epoch [50], masks_loss: 1.1450,masks_dice_score: nan,\n",
      "Epoch [51], masks_loss: 1.1452,masks_dice_score: nan,\n",
      "Epoch [52], masks_loss: 1.1453,masks_dice_score: nan,\n",
      "Epoch [53], masks_loss: 1.1453,masks_dice_score: nan,\n",
      "Epoch [54], masks_loss: 1.1456,masks_dice_score: nan,\n",
      "Epoch [55], masks_loss: 1.1457,masks_dice_score: nan,\n",
      "Epoch [56], masks_loss: 1.1450,masks_dice_score: nan,\n",
      "Epoch [57], masks_loss: 1.1454,masks_dice_score: nan,\n",
      "Epoch [58], masks_loss: 1.1447,masks_dice_score: nan,\n",
      "Epoch [59], masks_loss: 1.1448,masks_dice_score: nan,\n",
      "Epoch [60], masks_loss: 1.1453,masks_dice_score: nan,\n",
      "Epoch [61], masks_loss: 1.1446,masks_dice_score: nan,\n",
      "Epoch [62], masks_loss: 1.1447,masks_dice_score: nan,\n",
      "Epoch [63], masks_loss: 1.1447,masks_dice_score: nan,\n",
      "Epoch [64], masks_loss: 1.1449,masks_dice_score: nan,\n",
      "Epoch [65], masks_loss: 1.1450,masks_dice_score: nan,\n",
      "Epoch [66], masks_loss: 1.1461,masks_dice_score: nan,\n",
      "Epoch [67], masks_loss: 1.1452,masks_dice_score: nan,\n",
      "Epoch [68], masks_loss: 1.1453,masks_dice_score: nan,\n",
      "Epoch [69], masks_loss: 1.1450,masks_dice_score: nan,\n"
     ]
    }
   ],
   "source": [
    "# TODO: try using dice loss\n",
    "history = fit_seg(\n",
    "    model=model,\n",
    "    train_dataloader=train_dl,\n",
    "    validation_dataloader=val_dl,\n",
    "    epochs=num_epochs + 50,\n",
    "    lr=1e-5,\n",
    "    opt_func=optim.Adam,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_losses = [x[\"train_loss\"] for x in history]\n",
    "\n",
    "plt.plot(train_losses, \"-bx\")\n",
    "\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.grid()\n",
    "plt.legend([\"train_loss\"])\n",
    "plt.title(\"Loss vs. NO. of epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Example predictions (logits) and targets\n",
    "predictions = torch.randn(32, 3, 224, 224)\n",
    "targets = torch.randint(0, 3, (32, 224, 224), dtype=torch.long)\n",
    "\n",
    "targets.shape, predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the loss using the functional form\n",
    "loss = F.cross_entropy(predictions, targets)\n",
    "print(loss.item())\n",
    "\n",
    "# You can also specify arguments directly in the function call\n",
    "loss_with_reduction = F.cross_entropy(predictions, targets, reduction=\"sum\")\n",
    "print(loss_with_reduction.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
